{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edited just for getting a simple graph , rest is same . Ignore the 1st code block else rest is same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vaibhav/miniconda3/envs/chessgpt/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-06-01 02:10:25,871 - train_test_chess - INFO - Using device: cuda\n",
      "2024-06-01 02:10:25,872 - train_test_chess - INFO - {'vocab_size': 17, 'itos': {0: '\\n', 1: ' ', 2: '-', 3: '.', 4: '/', 5: '0', 6: '1', 7: '2', 8: '3', 9: '4', 10: '5', 11: '6', 12: '7', 13: '8', 14: '9', 15: ';', 16: 'x'}, 'stoi': {'\\n': 0, ' ': 1, '-': 2, '.': 3, '/': 4, '0': 5, '1': 6, '2': 7, '3': 8, '4': 9, '5': 10, '6': 11, '7': 12, '8': 13, '9': 14, ';': 15, 'x': 16}}\n",
      "2024-06-01 02:10:25,873 - train_test_chess - INFO - [6, 3, 1, 6, 6, 2, 6, 10, 1, 7, 9, 2, 7, 5, 1, 7, 3, 1, 13, 2, 6, 6, 1, 7, 13, 2, 7, 9]\n",
      "2024-06-01 02:10:25,874 - train_test_chess - INFO - Performing round trip test on meta\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from fancy_einsum import einsum\n",
    "import chess\n",
    "import numpy as np\n",
    "import pickle\n",
    "import logging\n",
    "import plotly.graph_objects as go\n",
    "from functools import partial\n",
    "\n",
    "import chess_utils\n",
    "import train_test_chess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a bunch of setup below to get some data in some tensors that we can feed to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7b02c25d4e50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flags to control logging\n",
    "debug_mode = False\n",
    "info_mode = True\n",
    "\n",
    "if debug_mode:\n",
    "    log_level = logging.DEBUG\n",
    "elif info_mode:\n",
    "    log_level = logging.INFO\n",
    "else:\n",
    "    log_level = logging.WARNING\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=log_level)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can select which probe and model to use. By default, the model_setup.py downloads a lichess 8 layer model. We can then select a probe from saved_probes/. Ideally, this should also be a lichess probe. Then this code should auto populate parameters according to the probe's state dict.\n",
    "\n",
    "To reproduce paper / blog post figures, set USE_16_LAYER to True and run model_setup.py on the lichess 16 layer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['linear_probe', 'final_loss', 'iters', 'epochs', 'acc', 'linear_probe_name', 'layer', 'indexing_function_name', 'batch_size', 'lr', 'wd', 'pos_start', 'num_epochs', 'num_games', 'modes', 'wandb_project', 'config_name', 'column_name', 'levels_of_interest', 'split', 'dataset_prefix', 'model_name', 'n_layers', 'wandb_run_name', 'player_color'])\n",
      "final_loss tensor(12.0749, requires_grad=True)\n",
      "iters 50000\n",
      "epochs 4\n",
      "acc tensor(0.9263)\n",
      "linear_probe_name checkers_piece_probe\n",
      "layer 6\n",
      "indexing_function_name find_dots_indices\n",
      "batch_size 2\n",
      "lr 0.001\n",
      "wd 0.01\n",
      "pos_start 0\n",
      "num_epochs 5\n",
      "num_games 10000\n",
      "modes 1\n",
      "wandb_project chess_linear_probes\n",
      "config_name checkers_piece_probe\n",
      "column_name None\n",
      "levels_of_interest None\n",
      "split train\n",
      "dataset_prefix checkers_\n",
      "model_name tf_lens_CheckersHuman\n",
      "n_layers 8\n",
      "wandb_run_name checkers_piece_probe_tf_lens_CheckersHuman_layer_6_indexing_find_dots_indices_max_games_10000\n",
      "player_color White\n",
      "ALWAYS HERE RIGHT?\n"
     ]
    }
   ],
   "source": [
    "MODEL_DIR = \"models/\"\n",
    "DATA_DIR = \"data/\"\n",
    "PROBE_DIR = \"linear_probes/\"\n",
    "SAVED_PROBE_DIR = \"linear_probes/saved_probes/\"\n",
    "SPLIT = \"test\"\n",
    "\n",
    "DEVICE = \"cpu\"\n",
    "logger.info(f\"Using device: {DEVICE}\")\n",
    "#LAYER = 5\n",
    "LAYER = 6\n",
    "#base_probe_name = \"tf_lens_lichess_8layers_ckpt_no_optimizer_chess_piece_probe_layer_0.pth\"\n",
    "#tf_lens_Checkers16M_checkers_piece_probe_layer_5.pth\n",
    "base_probe_name = \"tf_lens_CheckersHuman_checkers_piece_probe_layer_6.pth\"\n",
    "\n",
    "USE_16_LAYER = False\n",
    "\n",
    "if USE_16_LAYER:\n",
    "    LAYER = 11\n",
    "    base_probe_name = \"tf_lens_lichess_16layers_ckpt_no_optimizer_chess_piece_probe_layer_0.pth\"\n",
    "\n",
    "probe_to_test = base_probe_name.replace(\"layer_0\", f\"layer_{LAYER}\")\n",
    "\n",
    "num_games = 10\n",
    "sample_size = 1\n",
    "modes = 1\n",
    "\n",
    "probe_file_location = f\"{SAVED_PROBE_DIR}{probe_to_test}\"\n",
    "with open(probe_file_location, \"rb\") as f:\n",
    "    state_dict = torch.load(f, map_location=torch.device(DEVICE))\n",
    "    print(state_dict.keys())\n",
    "    for key in state_dict.keys():\n",
    "        if key != \"linear_probe\":\n",
    "            print(key, state_dict[key])\n",
    "\n",
    "    config = chess_utils.find_config_by_name(state_dict[\"config_name\"])\n",
    "    layer = state_dict[\"layer\"]\n",
    "    model_name = state_dict[\"model_name\"]\n",
    "    dataset_prefix = state_dict[\"dataset_prefix\"]\n",
    "    column_name = state_dict[\"column_name\"]\n",
    "    config.pos_start = state_dict[\"pos_start\"]\n",
    "    levels_of_interest = None\n",
    "    if \"levels_of_interest\" in state_dict.keys():\n",
    "        levels_of_interest = state_dict[\"levels_of_interest\"]\n",
    "    config.levels_of_interest = levels_of_interest\n",
    "    indexing_function_name = state_dict[\"indexing_function_name\"]\n",
    "    n_layers = state_dict[\"n_layers\"]\n",
    "    \n",
    "\n",
    "    split = SPLIT\n",
    "    input_dataframe_file = f\"{DATA_DIR}{dataset_prefix}{split}.csv\"\n",
    "    config = chess_utils.set_config_min_max_vals_and_column_name(\n",
    "        config, input_dataframe_file, dataset_prefix\n",
    "    )\n",
    "    misc_logging_dict = {\n",
    "        \"split\": split,\n",
    "        \"dataset_prefix\": dataset_prefix,\n",
    "        \"model_name\": model_name,\n",
    "        \"n_layers\": n_layers,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the below cell, we index at select 1 of the num_games. The reason we do this is that with a large number of games, storing all the resid_posts and state_stacks quickly grows to many gigabytes of VRAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for HookedTransformer:\n\tMissing key(s) in state_dict: \"blocks.4.attn.W_Q\", \"blocks.4.attn.W_K\", \"blocks.4.attn.W_V\", \"blocks.4.attn.W_O\", \"blocks.4.attn.b_Q\", \"blocks.4.attn.b_K\", \"blocks.4.attn.b_V\", \"blocks.4.attn.b_O\", \"blocks.4.attn.mask\", \"blocks.4.attn.IGNORE\", \"blocks.4.mlp.W_in\", \"blocks.4.mlp.b_in\", \"blocks.4.mlp.W_out\", \"blocks.4.mlp.b_out\", \"blocks.5.attn.W_Q\", \"blocks.5.attn.W_K\", \"blocks.5.attn.W_V\", \"blocks.5.attn.W_O\", \"blocks.5.attn.b_Q\", \"blocks.5.attn.b_K\", \"blocks.5.attn.b_V\", \"blocks.5.attn.b_O\", \"blocks.5.attn.mask\", \"blocks.5.attn.IGNORE\", \"blocks.5.mlp.W_in\", \"blocks.5.mlp.b_in\", \"blocks.5.mlp.W_out\", \"blocks.5.mlp.b_out\", \"blocks.6.attn.W_Q\", \"blocks.6.attn.W_K\", \"blocks.6.attn.W_V\", \"blocks.6.attn.W_O\", \"blocks.6.attn.b_Q\", \"blocks.6.attn.b_K\", \"blocks.6.attn.b_V\", \"blocks.6.attn.b_O\", \"blocks.6.attn.mask\", \"blocks.6.attn.IGNORE\", \"blocks.6.mlp.W_in\", \"blocks.6.mlp.b_in\", \"blocks.6.mlp.W_out\", \"blocks.6.mlp.b_out\", \"blocks.7.attn.W_Q\", \"blocks.7.attn.W_K\", \"blocks.7.attn.W_V\", \"blocks.7.attn.W_O\", \"blocks.7.attn.b_Q\", \"blocks.7.attn.b_K\", \"blocks.7.attn.b_V\", \"blocks.7.attn.b_O\", \"blocks.7.attn.mask\", \"blocks.7.attn.IGNORE\", \"blocks.7.mlp.W_in\", \"blocks.7.mlp.b_in\", \"blocks.7.mlp.W_out\", \"blocks.7.mlp.b_out\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m probe_data \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_chess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct_linear_probe_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_dataframe_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_games\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DEVICE \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     11\u001b[0m     probe_data\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcpu()\n",
      "File \u001b[0;32m~/nanogpt/CheckersGPT/chess_llm_interpretability/train_test_chess.py:647\u001b[0m, in \u001b[0;36mconstruct_linear_probe_data\u001b[0;34m(input_dataframe_file, dataset_prefix, n_layers, model_name, config, max_games, device)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstruct_linear_probe_data\u001b[39m(\n\u001b[1;32m    629\u001b[0m     input_dataframe_file: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    630\u001b[0m     dataset_prefix: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    635\u001b[0m     device: torch\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[1;32m    636\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LinearProbeData:\n\u001b[1;32m    637\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"We need the following data to train or test a linear probe:\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;124;03m    - The layer to probe in the GPT\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;124;03m    - The GPT model in transformer_lens format\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;124;03m    - skill_stack: the skill levels of the players in the games (only used if probing for skill)\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 647\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mget_transformer_lens_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    648\u001b[0m     user_state_dict_one_hot_mapping, df \u001b[38;5;241m=\u001b[39m process_dataframe(input_dataframe_file, config)\n\u001b[1;32m    649\u001b[0m     df \u001b[38;5;241m=\u001b[39m df[:max_games]\n",
      "File \u001b[0;32m~/nanogpt/CheckersGPT/chess_llm_interpretability/train_test_chess.py:126\u001b[0m, in \u001b[0;36mget_transformer_lens_model\u001b[0;34m(model_name, n_layers, device)\u001b[0m\n\u001b[1;32m    114\u001b[0m cfg \u001b[38;5;241m=\u001b[39m HookedTransformerConfig(\n\u001b[1;32m    115\u001b[0m     n_layers\u001b[38;5;241m=\u001b[39mn_layers,\n\u001b[1;32m    116\u001b[0m     d_model\u001b[38;5;241m=\u001b[39mD_MODEL,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m     normalization_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLNPre\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    124\u001b[0m )\n\u001b[1;32m    125\u001b[0m model \u001b[38;5;241m=\u001b[39m HookedTransformer(cfg)\n\u001b[0;32m--> 126\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mMODEL_DIR\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/chessgpt/lib/python3.11/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for HookedTransformer:\n\tMissing key(s) in state_dict: \"blocks.4.attn.W_Q\", \"blocks.4.attn.W_K\", \"blocks.4.attn.W_V\", \"blocks.4.attn.W_O\", \"blocks.4.attn.b_Q\", \"blocks.4.attn.b_K\", \"blocks.4.attn.b_V\", \"blocks.4.attn.b_O\", \"blocks.4.attn.mask\", \"blocks.4.attn.IGNORE\", \"blocks.4.mlp.W_in\", \"blocks.4.mlp.b_in\", \"blocks.4.mlp.W_out\", \"blocks.4.mlp.b_out\", \"blocks.5.attn.W_Q\", \"blocks.5.attn.W_K\", \"blocks.5.attn.W_V\", \"blocks.5.attn.W_O\", \"blocks.5.attn.b_Q\", \"blocks.5.attn.b_K\", \"blocks.5.attn.b_V\", \"blocks.5.attn.b_O\", \"blocks.5.attn.mask\", \"blocks.5.attn.IGNORE\", \"blocks.5.mlp.W_in\", \"blocks.5.mlp.b_in\", \"blocks.5.mlp.W_out\", \"blocks.5.mlp.b_out\", \"blocks.6.attn.W_Q\", \"blocks.6.attn.W_K\", \"blocks.6.attn.W_V\", \"blocks.6.attn.W_O\", \"blocks.6.attn.b_Q\", \"blocks.6.attn.b_K\", \"blocks.6.attn.b_V\", \"blocks.6.attn.b_O\", \"blocks.6.attn.mask\", \"blocks.6.attn.IGNORE\", \"blocks.6.mlp.W_in\", \"blocks.6.mlp.b_in\", \"blocks.6.mlp.W_out\", \"blocks.6.mlp.b_out\", \"blocks.7.attn.W_Q\", \"blocks.7.attn.W_K\", \"blocks.7.attn.W_V\", \"blocks.7.attn.W_O\", \"blocks.7.attn.b_Q\", \"blocks.7.attn.b_K\", \"blocks.7.attn.b_V\", \"blocks.7.attn.b_O\", \"blocks.7.attn.mask\", \"blocks.7.attn.IGNORE\", \"blocks.7.mlp.W_in\", \"blocks.7.mlp.b_in\", \"blocks.7.mlp.W_out\", \"blocks.7.mlp.b_out\". "
     ]
    }
   ],
   "source": [
    "probe_data = train_test_chess.construct_linear_probe_data(\n",
    "    input_dataframe_file,\n",
    "    dataset_prefix,\n",
    "    n_layers,\n",
    "    model_name,\n",
    "    config,\n",
    "    num_games,\n",
    "    DEVICE,\n",
    ")\n",
    "if DEVICE == \"cpu\":\n",
    "    probe_data.model.cpu()\n",
    "\n",
    "game_of_interest = 1 # 3\n",
    "\n",
    "game_length_in_chars = len(probe_data.board_seqs_string[0])\n",
    "\n",
    "\n",
    "state_stacks_all_chars = chess_utils.create_state_stacks(probe_data.board_seqs_string[:num_games], config.custom_board_state_function)\n",
    "logger.info(f\"state_stack shape: {state_stacks_all_chars.shape}\")\n",
    "assert(state_stacks_all_chars.shape) == (modes, num_games, game_length_in_chars, config.num_rows, config.num_cols)\n",
    "white_move_indices = probe_data.custom_indices[:num_games]\n",
    "print(white_move_indices.shape)\n",
    "num_white_moves = white_move_indices.shape[1]\n",
    "assert(white_move_indices.shape) == (num_games, num_white_moves)\n",
    "\n",
    "\n",
    "print(\"\\nSelecting the game of interest\")\n",
    "print(probe_data.board_seqs_int.shape)\n",
    "print(state_stacks_all_chars.shape)\n",
    "print(white_move_indices.shape)\n",
    "print(len(probe_data.board_seqs_string), len(probe_data.board_seqs_string[0]))\n",
    "\n",
    "probe_data.board_seqs_int = probe_data.board_seqs_int[game_of_interest].unsqueeze(0)\n",
    "probe_data.board_seqs_string = [probe_data.board_seqs_string[game_of_interest]]\n",
    "probe_data.custom_indices = white_move_indices[game_of_interest].unsqueeze(0)\n",
    "state_stacks_all_chars = state_stacks_all_chars[:, game_of_interest, :, :, :].unsqueeze(1)\n",
    "white_move_indices = white_move_indices[game_of_interest].unsqueeze(0)\n",
    "\n",
    "print(probe_data.board_seqs_int.shape)\n",
    "print(state_stacks_all_chars.shape)\n",
    "print(white_move_indices.shape)\n",
    "print(len(probe_data.board_seqs_string), len(probe_data.board_seqs_string[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an explanation of all the data we just generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important note: At the bottom of the below cell, I currently am using softmax to view probe output probabilities. You can comment that out to view raw logits instead.\n",
    "\n",
    "In this cell, we input the board_seqs_int to the GPT to obtain resid_post, the intermediate activations after our layer of interest. We index into resid_post using white_move_indices. These indexed resid_posts are then input to the linear probe, which outputs probe_out, a probability distribution for the state of every square on the board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(probe_file_location, map_location=torch.device(DEVICE))\n",
    "linear_probe = checkpoint[\"linear_probe\"]\n",
    "print(linear_probe.shape)\n",
    "\n",
    "\n",
    "one_hot_range = config.max_val - config.min_val + 1\n",
    "\n",
    "board_seqs_int = probe_data.board_seqs_int[:].to(DEVICE)\n",
    "assert(board_seqs_int.shape) == (1, game_length_in_chars)\n",
    "\n",
    "indexed_state_stacks = []\n",
    "\n",
    "for batch_idx in range(sample_size):\n",
    "    # Get the indices for the current batch\n",
    "    dots_indices_for_batch = white_move_indices[batch_idx]\n",
    "\n",
    "    # Index the state_stack for the current batch. Adding an unsqueeze operation to maintain the batch dimension.\n",
    "    indexed_state_stack = state_stacks_all_chars[:, batch_idx:batch_idx+1, dots_indices_for_batch, :, :]\n",
    "\n",
    "    # Append the result to the list\n",
    "    indexed_state_stacks.append(indexed_state_stack)\n",
    "\n",
    "# Concatenate the indexed state stacks along the second dimension (batch dimension)\n",
    "# Since we're maintaining the batch dimension during indexing, we don't need to add it back in.\n",
    "state_stack_white_moves = torch.cat(indexed_state_stacks, dim=1)\n",
    "\n",
    "print(\"state stack shapes\")\n",
    "print(state_stack_white_moves.shape)\n",
    "print(state_stacks_all_chars.shape)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    _, cache = probe_data.model.run_with_cache(board_seqs_int[:, :-1], return_type=None)\n",
    "    resid_post = cache[\"resid_post\", layer][:, :]\n",
    "\n",
    "# print(resid_post.shape)\n",
    "assert(resid_post.shape) == (sample_size, game_length_in_chars - 1, linear_probe.shape[1])\n",
    "\n",
    "# Initialize a list to hold the indexed state stacks\n",
    "indexed_resid_posts = []\n",
    "\n",
    "for batch_idx in range(sample_size):\n",
    "    # Get the indices for the current batch\n",
    "    dots_indices_for_batch = white_move_indices[batch_idx]\n",
    "\n",
    "    # Index the state_stack for the current batch\n",
    "    indexed_resid_post = resid_post[batch_idx, dots_indices_for_batch]\n",
    "\n",
    "    # Append the result to the list\n",
    "    indexed_resid_posts.append(indexed_resid_post)\n",
    "\n",
    "# Stack the indexed state stacks along the first dimension\n",
    "# This results in a tensor of shape [2, 61, 8, 8] (assuming all batches have 61 indices)\n",
    "resid_post = torch.stack(indexed_resid_posts)\n",
    "resid_post = resid_post.to(DEVICE)\n",
    "print(\"Resid post\", resid_post.shape)\n",
    "probe_out = einsum(\n",
    "    \"batch pos d_model, modes d_model rows cols options -> modes batch pos rows cols options\",\n",
    "    resid_post,\n",
    "    linear_probe,\n",
    ")\n",
    "probe_out = probe_out.log_softmax(-1)\n",
    "print(f\"Probe out shape: {probe_out.shape}\")\n",
    "assert(probe_out.shape) == (modes, sample_size, white_move_indices.shape[1], config.num_rows, config.num_cols, one_hot_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can select which move you want to visualize (move_of_interest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_of_interest = 11\n",
    "GAME_IDX = 0 # After refactoring to discard unused games, this is always 0\n",
    "move_of_interest_index = white_move_indices[GAME_IDX][move_of_interest] # Used to select pgn strings\n",
    "move_of_interest_state = state_stack_white_moves[0][GAME_IDX][move_of_interest]\n",
    "print(move_of_interest_state.shape)\n",
    "print(move_of_interest_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we one hot encode our move_of_interest and store it in move_of_interest_state_one_hot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_stacks_one_hot = chess_utils.state_stack_to_one_hot(modes, config.num_rows, config.num_cols, config.min_val, config.max_val, DEVICE, state_stack_white_moves)\n",
    "print(state_stacks_one_hot.shape)\n",
    "assert(state_stacks_one_hot.shape) == (modes, sample_size, num_white_moves, config.num_rows, config.num_cols, one_hot_range)\n",
    "move_of_interest_state_one_hot = state_stacks_one_hot[0][GAME_IDX][move_of_interest]\n",
    "print(move_of_interest_state_one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the argmax of each square's probe probability distribution and store it in state_stacks_probe_outputs for easy graphing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(move_of_interest_state_one_hot.shape)\n",
    "print(state_stacks_one_hot.shape)\n",
    "state_stacks_probe_outputs = chess_utils.one_hot_to_state_stack(probe_out, config.min_val)\n",
    "state_stacks_probe_outputs = torch.tensor(state_stacks_probe_outputs)\n",
    "print(state_stacks_probe_outputs.shape)\n",
    "assert(state_stacks_probe_outputs.shape) == (modes, sample_size, num_white_moves, config.num_rows, config.num_cols)\n",
    "print(state_stacks_probe_outputs[0][GAME_IDX][move_of_interest])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change blank_index, king_index, or pawn_index if you want to visualize the probe's view of other pieces. For example, if I want to see the black queen, I could set blank_index = -5 (refer to INT_TO_CHAR for the mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "INT_TO_CHAR = {\n",
    "    -6: \"\\u265a\",\n",
    "    -5: \"\\u265b\",\n",
    "    -4: \"\\u265c\",\n",
    "    -3: \"\\u265d\",\n",
    "    -2: \"\\u265e\",\n",
    "    -1: \"\\u265f\",\n",
    "    0: \".\",\n",
    "    1: \"\\u2659\",\n",
    "    2: \"\\u2658\",\n",
    "    3: \"\\u2657\",\n",
    "    4: \"\\u2656\",\n",
    "    5: \"\\u2655\",\n",
    "    6: \"\\u2654\",\n",
    "}\n",
    "\n",
    "# Mapping of integers to chess pieces\n",
    "# I'm duplicating this from chess_utils.py for easy reference\n",
    "# PIECE_TO_ONE_HOT_MAPPING = {\n",
    "#     -6: 0,\n",
    "#     -5: 1,\n",
    "#     -4: 2,\n",
    "#     -3: 3,\n",
    "#     -2: 4,\n",
    "#     -1: 5,\n",
    "#     0: 6,\n",
    "#     1: 7,\n",
    "#     2: 8,\n",
    "#     3: 9,\n",
    "#     4: 10,\n",
    "#     5: 11,\n",
    "#     6: 12,\n",
    "# }\n",
    "\n",
    "PIECE_TO_ONE_HOT_MAPPING = {\n",
    "    -1:0,\n",
    "    0:1,\n",
    "    1:2\n",
    "}\n",
    "\n",
    "#{-1: 0, 0: 1, 1: 2}\n",
    "\n",
    "# Mapping of chess pieces to integers\n",
    "PIECE_TO_INT = {\n",
    "    chess.PAWN: 1,\n",
    "    chess.KNIGHT: 2,\n",
    "    chess.BISHOP: 3,\n",
    "    chess.ROOK: 4,\n",
    "    chess.QUEEN: 5,\n",
    "    chess.KING: 6,\n",
    "}\n",
    "\n",
    "INT_TO_PIECE = {value: key for key, value in PIECE_TO_INT.items()}\n",
    "\n",
    "BLANK_INDEX = PIECE_TO_ONE_HOT_MAPPING[0]\n",
    "#white_pawn_index = PIECE_TO_ONE_HOT_MAPPING[1]\n",
    "#black_king_index = PIECE_TO_ONE_HOT_MAPPING[-6]\n",
    "white_pawn_index = PIECE_TO_ONE_HOT_MAPPING[1]\n",
    "black_king_index = PIECE_TO_ONE_HOT_MAPPING[-1]\n",
    "\n",
    "def plot_board_state(board_state: torch.Tensor, clip_size: int = 200, show_scale: bool = False):\n",
    "    # color scale: Black for -1, Gray for 0, White for 1\n",
    "    # colorscale = [[0.0, 'black'], [0.5, 'gray'], [1.0, 'white']]\n",
    "    colorscale = 'gray' #gray\n",
    "    if board_state.is_cuda:\n",
    "        board_state = board_state.cpu()\n",
    "    board_state = np.clip(board_state.numpy(), -clip_size, clip_size)\n",
    "\n",
    "    # Create heatmap\n",
    "    heatmap = go.Heatmap(z=board_state, colorscale=colorscale, showscale=show_scale)\n",
    "    return heatmap\n",
    "\n",
    "print(move_of_interest_state_one_hot[:, :, white_pawn_index])\n",
    "# heatmap = plot_board_state(move_of_interest_state_one_hot[:, :, white_pawn_index], show_scale=True)\n",
    "\n",
    "move_of_interest_probe_out = probe_out[0][0][move_of_interest]\n",
    "print(move_of_interest_probe_out.shape)\n",
    "\n",
    "heatmap = plot_board_state(move_of_interest_probe_out[:, :, white_pawn_index], show_scale=True)\n",
    "\n",
    "# Define the layout\n",
    "layout = go.Layout(\n",
    "    title=\"Chess board white pawns\",\n",
    "    xaxis=dict(ticks='', nticks=8),\n",
    "    yaxis=dict(ticks='', nticks=8),\n",
    "    autosize=False,\n",
    "    width=600,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "# Create figure and plot\n",
    "fig = go.Figure(data=[heatmap], layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_text(board_state: torch.Tensor) -> np.ndarray:\n",
    "    # Create a mapping from numbers to characters\n",
    "    # Update this mapping according to your requirements\n",
    "\n",
    "    # Convert the tensor to numpy array for easier processing\n",
    "    board_array = board_state.numpy()\n",
    "\n",
    "    # Create an empty array with the same shape for text\n",
    "    text_array = np.empty(board_array.shape, dtype=str)\n",
    "\n",
    "    # Fill the text array with corresponding characters\n",
    "    for i in range(board_array.shape[0]):\n",
    "        for j in range(board_array.shape[1]):\n",
    "            text_array[i, j] = INT_TO_CHAR.get(board_array[i, j], str(board_array[i, j]))\n",
    "\n",
    "    return text_array\n",
    "\n",
    "def plot_board_state_with_text(board_state: torch.Tensor):\n",
    "    # Convert the tensor to a text matrix\n",
    "    text_matrix = tensor_to_text(board_state)\n",
    "\n",
    "    # Define the custom colorscale\n",
    "    colorscale = [\n",
    "        [0, 'white'],   # Negative values\n",
    "        [0.49, 'white'],\n",
    "        [0.5, 'grey'],  # Zero\n",
    "        [0.51, 'white'],\n",
    "        [1, 'white']    # Positive values\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Create heatmap with text and custom colorscale\n",
    "    heatmap = go.Heatmap(\n",
    "        z=board_state.numpy(), \n",
    "        text=text_matrix, \n",
    "        showscale=False, \n",
    "        colorscale=colorscale,\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont=dict(size=48) \n",
    "    )\n",
    "\n",
    "    return heatmap\n",
    "heatmap = plot_board_state_with_text(move_of_interest_state)\n",
    "\n",
    "# Define the layout\n",
    "layout = go.Layout(\n",
    "    title=\"Chess board state with text\",\n",
    "    xaxis=dict(ticks='', nticks=8),\n",
    "    yaxis=dict(ticks='', nticks=8),\n",
    "    autosize=False,\n",
    "    width=600,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "# Create figure and plot\n",
    "fig = go.Figure(data=[heatmap], layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the tensors\n",
    "board_tensor_1 = move_of_interest_state\n",
    "board_tensor_2 = state_stacks_probe_outputs[0][0][move_of_interest]\n",
    "print(board_tensor_1)\n",
    "print(board_tensor_2)\n",
    "# Create a figure and two subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Plot the first game state\n",
    "axs[0].set_title('Ground Truth State', fontsize =30)\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        if (i + j) % 2 == 0:\n",
    "            color = 'white'\n",
    "        else:\n",
    "            color = 'gray'\n",
    "        axs[0].add_patch(plt.Rectangle((j, 7-i), 1, 1, color=color))\n",
    "\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            if board_tensor_1[i, j] == 1:\n",
    "                axs[0].add_patch(plt.Circle((j+0.5, 7-i+0.5), 0.3, color='white', ec='black'))\n",
    "            elif board_tensor_1[i, j] == -1:\n",
    "                axs[0].add_patch(plt.Circle((j+0.5, 7-i+0.5), 0.3, color='black', ec='black'))\n",
    "\n",
    "# Plot the second game state\n",
    "axs[1].set_title('Predicted State', fontsize =30)\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        if (i + j) % 2 == 0:\n",
    "            color = 'white'\n",
    "        else:\n",
    "            color = 'gray'\n",
    "        axs[1].add_patch(plt.Rectangle((j, 7-i), 1, 1, color=color))\n",
    "\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            if board_tensor_2[i, j] == 1:\n",
    "                axs[1].add_patch(plt.Circle((j+0.5, 7-i+0.5), 0.3, color='white', ec='black'))\n",
    "            elif board_tensor_2[i, j] == -1:\n",
    "                axs[1].add_patch(plt.Circle((j+0.5, 7-i+0.5), 0.3, color='black', ec='black'))\n",
    "\n",
    "# Set the limits and aspect ratio\n",
    "for ax in axs:\n",
    "    ax.set_xlim(0, 8)\n",
    "    ax.set_ylim(0, 8)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "# Remove axis ticks and labels\n",
    "for ax in axs:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "move_of_interest_probe_out = probe_out[0][0][move_of_interest]\n",
    "print(move_of_interest_probe_out.shape)\n",
    "\n",
    "fig_rows = 4\n",
    "fig_cols = 3\n",
    "fig = make_subplots(rows=fig_rows, cols=fig_cols, subplot_titles=[\n",
    "    \"Ground truth blank squares\", \"Predicted blank squares\", \"Confidence gradient blank squares\",\n",
    "    \"Ground truth white pawn positions\", \"Predicted white pawn positions\", \"Confidence gradient white pawn positions\",\n",
    "    \"Ground truth black king position\", \"Predicted black king position\", \"Confidence gradient black king position\",\n",
    "    \"Ground truth state\", \"Predicted board state\", \"Redundant probe output board state\"\n",
    "])\n",
    "\n",
    "\n",
    "# Specify the size of each plot\n",
    "plot_size = 400  # You can adjust this size\n",
    "\n",
    "fig.add_trace(plot_board_state(move_of_interest_state_one_hot[:, :, BLANK_INDEX]), row=1, col=1)\n",
    "fig.add_trace(plot_board_state(move_of_interest_probe_out[:, :, BLANK_INDEX], clip_size=2), row=1, col=2)\n",
    "fig.add_trace(plot_board_state(move_of_interest_probe_out[:, :, BLANK_INDEX]), row=1, col=3)\n",
    "\n",
    "fig.add_trace(plot_board_state(move_of_interest_state_one_hot[:, :, white_pawn_index]), row=2, col=1)\n",
    "fig.add_trace(plot_board_state(move_of_interest_probe_out[:, :, white_pawn_index], clip_size=2), row=2, col=2)\n",
    "fig.add_trace(plot_board_state(move_of_interest_probe_out[:, :, white_pawn_index], show_scale=True), row=2, col=3)\n",
    "\n",
    "fig.add_trace(plot_board_state(move_of_interest_state_one_hot[:, :, black_king_index]), row=3, col=1)\n",
    "fig.add_trace(plot_board_state(move_of_interest_probe_out[:, :, black_king_index], clip_size=2), row=3, col=2)\n",
    "fig.add_trace(plot_board_state(move_of_interest_probe_out[:, :, black_king_index]), row=3, col=3)\n",
    "\n",
    "fig.add_trace(plot_board_state_with_text(move_of_interest_state), row=4, col=1)\n",
    "fig.add_trace(plot_board_state_with_text(state_stacks_probe_outputs[0][0][move_of_interest]), row=4, col=2)\n",
    "fig.add_trace(plot_board_state_with_text(state_stacks_probe_outputs[0][0][move_of_interest]), row=4, col=2)\n",
    "\n",
    "# Adjust the overall size of the figure\n",
    "fig.update_layout(height=fig_rows * plot_size * 1.3, width=fig_cols * plot_size)\n",
    "fig.update_annotations(dict(font=dict(size=18))) \n",
    "\n",
    "\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "plot_size = 400  # You can adjust this size\n",
    "\n",
    "fig_rows = 1\n",
    "fig_cols = 2\n",
    "fig = make_subplots(rows=fig_rows, cols=fig_cols, subplot_titles=[\n",
    "    \"Ground truth black positions\", \"Predicted black positions\"\n",
    "])\n",
    "\n",
    "fig.add_trace(plot_board_state(move_of_interest_state_one_hot[:, :, white_pawn_index]), row=1, col=1)\n",
    "fig.add_trace(plot_board_state(move_of_interest_probe_out[:, :, white_pawn_index], clip_size=2), row=1, col=2)\n",
    "\n",
    "print(move_of_interest_state_one_hot[:, :, white_pawn_index])\n",
    "print(move_of_interest_probe_out[:, :, white_pawn_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_rows = 1\n",
    "fig_cols = 2\n",
    "fig = make_subplots(rows=fig_rows, cols=fig_cols, subplot_titles=[\n",
    "    \"Ground truth white position\", \"Predicted white position\"\n",
    "])\n",
    "\n",
    "fig.add_trace(plot_board_state(move_of_interest_state_one_hot[:, :, black_king_index]), row=1, col=1)\n",
    "fig.add_trace(plot_board_state(move_of_interest_probe_out[:, :, black_king_index], clip_size=2), row=1, col=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_rows = 1\n",
    "fig_cols = 2\n",
    "fig = make_subplots(rows=fig_rows, cols=fig_cols, subplot_titles=[\n",
    "    \"Ground truth state\", \"Predicted board state\"\n",
    "])\n",
    "\n",
    "\n",
    "# fig.add_trace(plot_board_state_with_text(move_of_interest_state), row=1, col=1)\n",
    "# fig.add_trace(plot_board_state_with_text(state_stacks_probe_outputs[0][0][move_of_interest]), row=1, col=2)\n",
    "\n",
    "fig.add_trace(plot_board_state(move_of_interest_state), row=1, col=1)\n",
    "fig.add_trace(plot_board_state(state_stacks_probe_outputs[0][0][move_of_interest]), row=1, col=2)\n",
    "\n",
    "# print(state_stacks_probe_outputs[0][0][move_of_interest])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the tensors\n",
    "board_tensor_1 = move_of_interest_state\n",
    "board_tensor_2 = state_stacks_probe_outputs[0][0][move_of_interest]\n",
    "print(board_tensor_1)\n",
    "print(board_tensor_2)\n",
    "# Create a figure and two subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Plot the first game state\n",
    "axs[0].set_title('Ground Truth State', fontsize =30)\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        if (i + j) % 2 == 0:\n",
    "            color = 'white'\n",
    "        else:\n",
    "            color = 'gray'\n",
    "        axs[0].add_patch(plt.Rectangle((j, 7-i), 1, 1, color=color))\n",
    "\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            if board_tensor_1[i, j] == 1:\n",
    "                axs[0].add_patch(plt.Circle((j+0.5, 7-i+0.5), 0.3, color='white', ec='black'))\n",
    "            elif board_tensor_1[i, j] == -1:\n",
    "                axs[0].add_patch(plt.Circle((j+0.5, 7-i+0.5), 0.3, color='black', ec='black'))\n",
    "\n",
    "# Plot the second game state\n",
    "axs[1].set_title('Predicted State', fontsize =30)\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        if (i + j) % 2 == 0:\n",
    "            color = 'white'\n",
    "        else:\n",
    "            color = 'gray'\n",
    "        axs[1].add_patch(plt.Rectangle((j, 7-i), 1, 1, color=color))\n",
    "\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            if board_tensor_2[i, j] == 1:\n",
    "                axs[1].add_patch(plt.Circle((j+0.5, 7-i+0.5), 0.3, color='white', ec='black'))\n",
    "            elif board_tensor_2[i, j] == -1:\n",
    "                axs[1].add_patch(plt.Circle((j+0.5, 7-i+0.5), 0.3, color='black', ec='black'))\n",
    "\n",
    "# Set the limits and aspect ratio\n",
    "for ax in axs:\n",
    "    ax.set_xlim(0, 8)\n",
    "    ax.set_ylim(0, 8)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "# Remove axis ticks and labels\n",
    "for ax in axs:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will check the percentage of squares in the sample (sample_size defaults to 1 game) where the ground truth matches the probe output.\n",
    "I also do a round trip through all the transformations, which should match 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_matching_percentage(state_stacks: torch.Tensor, probe_outputs: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the percentage of matching cells in two tensors.\n",
    "\n",
    "    :param state_stacks: A tensor of shape [1, 1, 680, 8, 8].\n",
    "    :param probe_outputs: A tensor of shape [1, 1, 680, 8, 8].\n",
    "    :return: The percentage of cells that match.\n",
    "    \"\"\"\n",
    "    # Element-wise comparison\n",
    "    matches = state_stacks == probe_outputs\n",
    "\n",
    "    # Count the number of matches\n",
    "    num_matches = matches.sum().item()\n",
    "\n",
    "    # Total number of elements\n",
    "    total_elements = state_stacks.numel()\n",
    "\n",
    "    # Calculate percentage\n",
    "    percentage = (num_matches / total_elements) * 100\n",
    "    print(f\"Out of {total_elements} elements, {num_matches} matched, {percentage}%\")\n",
    "\n",
    "    return percentage\n",
    "assert(state_stacks_probe_outputs.shape) == (state_stack_white_moves.shape)\n",
    "print(\"Linear probe accuracy on all board squares in sample size:\", calculate_matching_percentage(state_stack_white_moves, state_stacks_probe_outputs))\n",
    "\n",
    "round_trip = chess_utils.one_hot_to_state_stack(chess_utils.state_stack_to_one_hot(modes, config.num_rows, config.num_cols, config.min_val,config.max_val, DEVICE, state_stack_white_moves), config.min_val)\n",
    "round_trip = torch.tensor(round_trip)\n",
    "print(round_trip.shape)\n",
    "print(state_stack_white_moves.shape)\n",
    "assert(round_trip.shape) == (modes, sample_size, num_white_moves, config.num_rows, config.num_cols)\n",
    "assert(round_trip.shape) == state_stack_white_moves.shape\n",
    "matching_percentage = calculate_matching_percentage(round_trip, state_stack_white_moves)\n",
    "assert(matching_percentage == 100.0)\n",
    "print(f\"Round trip matching percentage: {matching_percentage}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can perform interventions on the model's internals and view the modified probe outputs. We can also verify the model produces legal moves under the modified state of the board.\n",
    "\n",
    "First, we perform a sanity check to ensure that our interventions on model activations are working correctly. In this case, diff should roughly equal flip_dir.\n",
    "\n",
    "Note that I'm only intervening on one layer here. By modifying the first for loop and training additional probes, we can easily intervene on an arbitrary amount of layers. If we were to intervene on multiple layers, we can only check that torch.allclose(diff, flip_dirs[layer], atol=1e-6) for the first layer that we intervene on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_data.model.reset_hooks()\n",
    "\n",
    "_, cache = probe_data.model.run_with_cache(board_seqs_int.to(DEVICE)[:, :-1], return_type=None)\n",
    "resid_post = cache[\"resid_post\", layer][:, :]\n",
    "\n",
    "r = 0\n",
    "c = 0\n",
    "\n",
    "probe_names = {}\n",
    "for i in range(layer, layer + 1):\n",
    "    probe_names[i] = base_probe_name.replace(\"layer_0\", f\"layer_{i}\")\n",
    "\n",
    "probes = {}\n",
    "\n",
    "# Use this to intervene on multiple layers\n",
    "for layer, probe_name in probe_names.items():\n",
    "    probe_file_location = f\"{SAVED_PROBE_DIR}{probe_name}\"\n",
    "    checkpoint = torch.load(probe_file_location, map_location=torch.device(DEVICE))\n",
    "    linear_probe = checkpoint[\"linear_probe\"]\n",
    "    probes[layer] = linear_probe\n",
    "\n",
    "\n",
    "flip_dirs = {}\n",
    "\n",
    "piece1 = BLANK_INDEX\n",
    "piece2 = black_king_index\n",
    "\n",
    "for layer, linear_probe in probes.items():\n",
    "    piece1_probe = linear_probe[:, :, r, c, piece1].squeeze()\n",
    "    piece2_probe = linear_probe[:, :, r, c, piece2].squeeze()\n",
    "    flip_dir = piece2_probe - piece1_probe\n",
    "    flip_dir.to(DEVICE)\n",
    "    flip_dirs[layer] = flip_dir\n",
    "\n",
    "def flip_hook(resid, hook, flip_dir: torch.Tensor):\n",
    "    resid[GAME_IDX, :] -= flip_dir # NOTE: We could only intervene on a single position in the sequence, but there's no harm in intervening on all of them\n",
    "\n",
    "probe_data.model.reset_hooks()\n",
    "\n",
    "for layer, flip_dir in flip_dirs.items():\n",
    "    temp_hook_fn = partial(flip_hook, flip_dir=flip_dir)\n",
    "    hook_name = f\"blocks.{layer}.hook_resid_post\"\n",
    "    probe_data.model.add_hook(hook_name, temp_hook_fn)\n",
    "\n",
    "print(probe_data.model.cpu())\n",
    "_, modified_cache = probe_data.model.run_with_cache(board_seqs_int.to(DEVICE)[:, :-1])\n",
    "probe_data.model.reset_hooks()\n",
    "modified_resid_post = modified_cache[\"resid_post\", layer][:, :]\n",
    "\n",
    "print(resid_post.shape)\n",
    "print(modified_resid_post.shape)\n",
    "\n",
    "diff = resid_post[GAME_IDX, 10, :] - modified_resid_post[GAME_IDX, 10, :]\n",
    "\n",
    "assert torch.allclose(diff, flip_dirs[layer], atol=1e-6)\n",
    "print(\"Flip hook test passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the model's vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/meta.pkl\", \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi = meta[\"stoi\"]\n",
    "itos = meta[\"itos\"]\n",
    "def encode_string(s: str) -> list[int]:\n",
    "    \"\"\"Encode a string into a list of integers.\"\"\"\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "\n",
    "def decode_list(l: list[int]) -> str:\n",
    "    \"\"\"Decode a list of integers into a string.\"\"\"\n",
    "    return \"\".join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we generate 10 characters using the model to determine the model's next move. Note that we are using argmax instead of a temperature based approach, so this will always return the most likely move.One annoying problem we deal with: In chess, the 0th row is at the bottom, which is how print(chess_board) displays everything. But, for our state stack (and any array), the 0th row is at the top.\n",
    "\n",
    "Now, we get a pgn string up to the current move and convert it to a chess board. We use it to create an encoded model_input as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(move_of_interest_state)\n",
    "\n",
    "pgn_string = probe_data.board_seqs_string[GAME_IDX][:move_of_interest_index + 1]\n",
    "model_input = encode_string(pgn_string)\n",
    "model_input = torch.tensor(model_input).unsqueeze(0).to(DEVICE)\n",
    "print(model_input.shape)\n",
    "board = chess_utils.pgn_string_to_board(pgn_string)\n",
    "\n",
    "print(board)\n",
    "print(board.legal_moves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a move using the model on the original board and check that the move is legal. Next, we determine which piece was moved, and which row / column the source square of the move was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_move = chess_utils.get_model_move(probe_data.model, meta, model_input)\n",
    "model_move_san = board.parse_san(model_move)\n",
    "assert model_move_san in board.legal_moves\n",
    "\n",
    "moved_piece = board.piece_at(model_move_san.from_square)\n",
    "moved_piece_int = PIECE_TO_INT[moved_piece.piece_type]\n",
    "moved_piece_probe_index = PIECE_TO_ONE_HOT_MAPPING[moved_piece_int]\n",
    "source_square = chess.square_name(model_move_san.from_square)\n",
    "\n",
    "\n",
    "r, c = chess_utils.square_to_coordinate(model_move_san.from_square)\n",
    "print(r, c)\n",
    "\n",
    "print(f\"Model move: {model_move_san}, moved piece: {moved_piece}, moved piece int: {moved_piece_int}, moved piece probe index: {moved_piece_probe_index}, source square: {source_square}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a modified board where the source square of the model's original move is blank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_state_stack = state_stack_white_moves.clone()\n",
    "modified_state_stack[0, GAME_IDX, move_of_interest, r, c] = 0\n",
    "modified_move_of_interest_state = modified_state_stack[0, GAME_IDX, move_of_interest]\n",
    "modified_state_stacks_one_hot = chess_utils.state_stack_to_one_hot(modes, config.num_rows, config.num_cols, config.min_val, config.max_val, DEVICE, modified_state_stack)\n",
    "modified_move_of_interest_state_one_hot = modified_state_stacks_one_hot[0][GAME_IDX][move_of_interest]\n",
    "modified_board = board.copy()\n",
    "modified_board.set_piece_at(model_move_san.from_square, None)\n",
    "print(modified_board)\n",
    "print(modified_board.legal_moves)\n",
    "\n",
    "assert modified_move_of_interest_state_one_hot.shape == move_of_interest_state_one_hot.shape\n",
    "assert modified_state_stack.shape == state_stack_white_moves.shape\n",
    "assert modified_state_stacks_one_hot.shape == state_stacks_one_hot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we get flip_dir, which is a probe of piece * piece_coefficient - blank square * blank_coefficient. In practice, I find that it works best when blank_coefficient is 0. We subtract this flip_dir from the model's activations at every token. We generate 10 new characters using the model, and verify that the new move under this modified state is legal according to the modified state. We also save a copy of the modified activations and generate modified probe outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = probe_data.model.run_with_cache(board_seqs_int.to(DEVICE)[:, :-1], return_type=None)\n",
    "resid_post = cache[\"resid_post\", layer][:, :]\n",
    "\n",
    "flip_dirs = {}\n",
    "\n",
    "piece1 = BLANK_INDEX\n",
    "piece1_probe = linear_probe[:, :, r, c, piece1].squeeze()\n",
    "piece2 = moved_piece_probe_index\n",
    "\n",
    "for layer, linear_probe in probes.items():\n",
    "    piece2_probe = linear_probe[:, :, r, c, piece2].squeeze()\n",
    "    flip_dir = piece2_probe - piece1_probe\n",
    "    flip_dir.to(DEVICE)\n",
    "    flip_dirs[layer] = flip_dir\n",
    "\n",
    "def flip_hook(resid, hook, flip_dir: torch.Tensor):\n",
    "    # print(resid[0, move_of_interest_index, :].shape)\n",
    "    # print(flip_dir.shape)\n",
    "    # print(piece1_probe.shape)\n",
    "    # left_side = torch.dot(resid[0, move_of_interest_index, :], piece1_probe) - 3.0\n",
    "    # right_side = torch.dot(flip_dir, piece1_probe)\n",
    "    # scale = left_side / right_side\n",
    "    # print(scale)\n",
    "    \n",
    "    # # Calculate scale\n",
    "    # scale = left_side / right_side\n",
    "    piece_coefficient = 1.0\n",
    "    blank_coefficient = 0.0\n",
    "    blank_probe = probes[layer][:, :, r, c, BLANK_INDEX].squeeze()\n",
    "    piece_probe = probes[layer][:, :, r, c, moved_piece_probe_index].squeeze()\n",
    "\n",
    "    flip_dir = (piece_probe * piece_coefficient) - (blank_probe * blank_coefficient)\n",
    "    flip_dir = flip_dir / flip_dir.norm()\n",
    "    scale = 1.0\n",
    "    resid[0, :] -= scale * flip_dir # NOTE: We could only intervene on a single position in the sequence, but there's no harm in intervening on all of them\n",
    "\n",
    "probe_data.model.reset_hooks()\n",
    "\n",
    "for layer, flip_dir in flip_dirs.items():\n",
    "    temp_hook_fn = partial(flip_hook, flip_dir=flip_dir)\n",
    "    hook_name = f\"blocks.{layer}.hook_resid_post\"\n",
    "    probe_data.model.add_hook(hook_name, temp_hook_fn)\n",
    "_, modified_cache = probe_data.model.run_with_cache(board_seqs_int.to(DEVICE)[:, :-1])\n",
    "modified_board_model_move = chess_utils.get_model_move(probe_data.model, meta, model_input)\n",
    "probe_data.model.reset_hooks()\n",
    "modified_resid_post = modified_cache[\"resid_post\", layer][:, :]\n",
    "\n",
    "\n",
    "print(modified_board_model_move)\n",
    "# modified_board_model_move_san = modified_board.parse_san(modified_board_model_move)\n",
    "# assert modified_board_model_move_san in modified_board.legal_moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(flip_dirs[layer].shape)\n",
    "print(resid_post.shape)\n",
    "print(modified_resid_post.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_modified_resid_posts = []\n",
    "\n",
    "for batch_idx in range(white_move_indices.size(0)):\n",
    "    dots_indices_for_batch = white_move_indices[batch_idx]\n",
    "    indexed_modified_resid_post = modified_resid_post[batch_idx, dots_indices_for_batch]\n",
    "    indexed_modified_resid_posts.append(indexed_modified_resid_post)\n",
    "\n",
    "# Stack the indexed state stacks along the first dimension\n",
    "stacked_modified_resid_post = torch.stack(indexed_modified_resid_posts)\n",
    "stacked_modified_resid_post = stacked_modified_resid_post.to(DEVICE)\n",
    "\n",
    "assert stacked_modified_resid_post.shape == (sample_size, num_white_moves, linear_probe.shape[1])\n",
    "\n",
    "modified_probe_out = einsum(\n",
    "    \"batch pos d_model, modes d_model rows cols options -> modes batch pos rows cols options\",\n",
    "    stacked_modified_resid_post,\n",
    "    linear_probe,\n",
    ")\n",
    "modified_state_stacks_probe_outputs = chess_utils.one_hot_to_state_stack(modified_probe_out, config.min_val)\n",
    "modified_state_stacks_probe_outputs = torch.tensor(modified_state_stacks_probe_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can graph the original and modified board states and probe outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "move_of_interest_probe_out = probe_out[0][0][move_of_interest]\n",
    "move_of_interest_probe_out_modified = modified_probe_out[0][0][move_of_interest]\n",
    "print(move_of_interest_probe_out.shape)\n",
    "\n",
    "fig_rows = 6\n",
    "fig_cols = 3\n",
    "fig = make_subplots(rows=fig_rows, cols=fig_cols, subplot_titles=[\n",
    "    \"Chess board blank squares\", \"Probe output blank squares clip=2\", \"Probe output blank squares no clipping\",\n",
    "    \"Chess board original piece\", \"Probe output original piece clip=5\", \"Probe output original piece no clipping\",\n",
    "    \"Modified chess board blank squares\", \"Probe output blank squares clip=2\", \"Probe output blank squares no clipping\",\n",
    "    \"Modified chess board original piece\", \"Probe output original piece clip=5\", \"Probe output original piece no clipping\",\n",
    "    \"Chess board state\", \"Probe output board state\", \"Redundant probe output board state\",\n",
    "    \"Modified chess board state\", \"Probe output board state\", \"Redundant probe output board state\"\n",
    "])\n",
    "\n",
    "\n",
    "# Specify the size of each plot\n",
    "plot_size = 400  # You can adjust this size\n",
    "\n",
    "\n",
    "\n",
    "fig.add_trace(plot_board_state(move_of_interest_state_one_hot[:, :, BLANK_INDEX]), row=1, col=1)\n",
    "fig.add_trace(plot_board_state(move_of_interest_probe_out[:, :, BLANK_INDEX], clip_size=2), row=1, col=2)\n",
    "fig.add_trace(plot_board_state(move_of_interest_probe_out[:, :, BLANK_INDEX]), row=1, col=3)\n",
    "\n",
    "fig.add_trace(plot_board_state(move_of_interest_state_one_hot[:, :, moved_piece_probe_index]), row=2, col=1)\n",
    "fig.add_trace(plot_board_state(move_of_interest_probe_out[:, :, moved_piece_probe_index], clip_size=5), row=2, col=2)\n",
    "fig.add_trace(plot_board_state(move_of_interest_probe_out[:, :, moved_piece_probe_index]), row=2, col=3)\n",
    "\n",
    "fig.add_trace(plot_board_state(modified_move_of_interest_state_one_hot[:, :, BLANK_INDEX]), row=3, col=1)\n",
    "fig.add_trace(plot_board_state(move_of_interest_probe_out_modified[:, :, BLANK_INDEX], clip_size=2), row=3, col=2)\n",
    "fig.add_trace(plot_board_state(move_of_interest_probe_out_modified[:, :, BLANK_INDEX]), row=3, col=3)\n",
    "\n",
    "fig.add_trace(plot_board_state(modified_move_of_interest_state_one_hot[:, :, moved_piece_probe_index]), row=4, col=1)\n",
    "fig.add_trace(plot_board_state(move_of_interest_probe_out_modified[:, :, moved_piece_probe_index], clip_size=5), row=4, col=2)\n",
    "fig.add_trace(plot_board_state(move_of_interest_probe_out_modified[:, :, moved_piece_probe_index]), row=4, col=3)\n",
    "\n",
    "fig.add_trace(plot_board_state_with_text(move_of_interest_state), row=5, col=1)\n",
    "fig.add_trace(plot_board_state_with_text(state_stacks_probe_outputs[0][0][move_of_interest]), row=5, col=2)\n",
    "fig.add_trace(plot_board_state_with_text(state_stacks_probe_outputs[0][0][move_of_interest]), row=5, col=2)\n",
    "\n",
    "fig.add_trace(plot_board_state_with_text(modified_move_of_interest_state), row=6, col=1)\n",
    "fig.add_trace(plot_board_state_with_text(modified_state_stacks_probe_outputs[0][0][move_of_interest]), row=6, col=2)\n",
    "fig.add_trace(plot_board_state_with_text(modified_state_stacks_probe_outputs[0][0][move_of_interest]), row=6, col=2)\n",
    "\n",
    "# Adjust the overall size of the figure\n",
    "fig.update_layout(height=fig_rows * plot_size, width=fig_cols * plot_size)\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "othello",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
